# Minimal FAIRChem v2 training config for IQA PKL energy regression
# Run: fairchem -c configs/train_iqa_energy_v2.yaml

defaults:
  - _self_

normalizer_rmsd: 1.423
# --------------------------------
# Experiment variables (all referenced below)
# --------------------------------
data:
  tasks_list:
    - _target_: fairchem.core.units.mlip_unit.mlip_unit.Task
      name: energy # task name
      level: system # system-level (not per-atom)
      property: energy # what weâ€™re learning
      out_spec: # how to find preds & labels
        energy:
          prediction_key: energy # head output key
          label_key: energy # from your key_mapping: {energy: e_total}
      loss_fn:
        _target_: torch.nn.MSELoss
        #_partial_: true
      normalizer: 
        _target_: fairchem.core.modules.normalization.normalizer.Normalizer
        mean: 0.0
        rmsd: ${normalizer_rmsd}
      datasets: ${runner.train_eval_unit.model.backbone.dataset_list}
      metrics: [] # optional
      train_on_free_atoms: false
      eval_on_free_atoms: false
      inference_only: false
  pass_through_head_outputs: false
  regress_stress: false

  # dataset kwargs used below
  train_dataset:
    _target_: fairchem.core.datasets.iqa_pkl_dataset.IQAPKLDataset
    src: /home/lukas/uni/whk/fairchem-test/dataset # <-- change me
    enforce_consistent_keys: true
    # If your PKL energy key is 'e_total', expose an alias 'energy' here:
    key_mapping:
      energy: e_total
  val_dataset:
    _target_: fairchem.core.datasets.iqa_pkl_dataset.IQAPKLDataset
    src: /home/lukas/uni/whk/fairchem-test/dataset # <-- change me
    enforce_consistent_keys: true
    key_mapping:
      energy: e_total

# dataloader knobs (referenced)
batch_size: 8
num_workers: 0
max_neighbors: 128

# training knobs (referenced)
epochs: 100
steps: null
lr: 1.0e-4
weight_decay: 1.0e-5
#evaluate_every_n_steps: 50
#checkpoint_every_n_steps: 500

# --------------------------------
# Required top-level keys for FAIRChem CLI
# --------------------------------
job:
  device_type: CPU
  debug: true
  scheduler:
    mode: LOCAL
    ranks_per_node: 1
    num_nodes: 1
  run_dir: ./runs/iqa_energy
  run_name: iqa_energy_smoketest
  logger:
    _target_: fairchem.core.common.logger.WandBSingletonLogger.init_wandb
    _partial_: true
    entity: example
    project: iqa_energy_debug

# Instantiate datasets (Hydra will call these)
train_dataset: ${data.train_dataset}
val_dataset: ${data.val_dataset}

train_dataloader:
  _target_: fairchem.core.components.common.dataloader_builder.get_dataloader
  dataset: ${train_dataset}
  batch_sampler_fn:
    _target_: fairchem.core.common.data_parallel.BalancedBatchSampler
    _partial_: true
    batch_size: ${batch_size}
    shuffle: true
    seed: 0
  num_workers: ${num_workers}
  collate_fn:
    _target_: fairchem.core.datasets.data_list_collater
    _partial_: true
eval_dataloader:
  _target_: fairchem.core.components.common.dataloader_builder.get_dataloader
  dataset: ${val_dataset}
  batch_sampler_fn:
    _target_: fairchem.core.common.data_parallel.BalancedBatchSampler
    _partial_: true
    batch_size: ${batch_size}
    shuffle: true
    seed: 0
  num_workers: ${num_workers}
  collate_fn:
    _target_: fairchem.core.datasets.data_list_collater
    _partial_: true
# Wire everything through the runner
runner:
  _target_: fairchem.core.components.train.train_runner.TrainEvalRunner
  max_epochs: ${epochs}
  max_steps: ${steps}
  train_dataloader: ${train_dataloader}
  eval_dataloader: ${eval_dataloader}
  train_eval_unit:
    _target_: fairchem.core.units.mlip_unit.mlip_unit.MLIPTrainEvalUnit
    job_config: ${job}
    tasks: ${data.tasks_list}

    model:
      _target_: fairchem.core.models.base.HydraModel
      backbone:
        model: fairchem.core.models.uma.escn_md.eSCNMDBackbone
        dataset_list: ["iqa_pkl"] # <-- REQUIRED by eSCNMDBackbone
        lmax: 2
        mmax: 2
        sphere_channels: 64
        num_layers: 3
        cutoff: 6.0
        max_neighbors: ${max_neighbors}
        use_pbc: false
      heads:
        energy:
          module: fairchem.core.models.uma.escn_md.IQA_Energy_Head
          reduce: sum

    optimizer_fn:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: ${lr}
      weight_decay: ${weight_decay}

    cosine_lr_scheduler_fn:
      _target_: fairchem.core.units.mlip_unit.mlip_unit._get_consine_lr_scheduler
      _partial_: true
      warmup_factor: 0.2
      warmup_epochs: 0.01
      lr_min_factor: 0.01
      epochs: ${epochs}
      steps: ${steps}
  callbacks:
    - _target_: fairchem.core.components.train.train_runner.TrainCheckpointCallback
      checkpoint_every_n_steps: 500
      max_saved_checkpoints: 3
    - _target_: torchtnt.framework.callbacks.TQDMProgressBar
